{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e2O4uXnhTya"
      },
      "source": [
        "# El arte de la imaginaci√≥n mecanizada - Ejercicio 1\n",
        "\n",
        "#### Pr√°cticas y pensamiento art√≠sticos en torno al giro tecnol√≥gico y a Inteligencias Artificiales, Bilbo, 11 de julio de 2023\n",
        "\n",
        "Basado muy fuertemente en [Grokking Stable Diffusion](https://colab.research.google.com/drive/1dlgggNa5Mz8sEAGU0wFCHhGLFooW_pf1?usp=sharing) de [Jonathan Whitaker](https://github.com/johnowhitaker) y en [Stable Diffusion with üß® diffusers](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb) de [ü§ó Hugging Face](https://github.com/huggingface/diffusers)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NLBlDaLyUKDe"
      },
      "outputs": [],
      "source": [
        "#@markdown #Comprobar GPU\n",
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9vzfOZ27UsTm"
      },
      "outputs": [],
      "source": [
        "#@markdown #Librer√≠as + Modelos + Funciones de ayuda\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "!pip install diffusers==0.3.0\n",
        "!pip install transformers scipy ftfy\n",
        "\n",
        "from google.colab import files\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from tqdm.auto import tqdm\n",
        "from torch import autocast\n",
        "from PIL import Image\n",
        "from huggingface_hub import notebook_login\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n",
        "import torch\n",
        "import re\n",
        "clear_output()\n",
        "\n",
        "\n",
        "vae = AutoencoderKL.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\", subfolder=\"vae\")\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "unet = UNet2DConditionModel.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\", subfolder=\"unet\")\n",
        "scheduler = LMSDiscreteScheduler(\n",
        "    beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
        "clear_output()\n",
        "\n",
        "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "vae = vae.to(torch_device)\n",
        "text_encoder = text_encoder.to(torch_device)\n",
        "unet = unet.to(torch_device)\n",
        "\n",
        "\n",
        "def latents_to_pil(latents):\n",
        "    latents = (1 / 0.18215) * latents\n",
        "    with torch.no_grad():\n",
        "        images = vae.decode(latents).sample\n",
        "    images = (images / 2 + 0.5).clamp(0, 1)\n",
        "    images = images.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "    images = (images * 255).round().astype(\"uint8\")\n",
        "    pil_images = [Image.fromarray(image) for image in images]\n",
        "\n",
        "    return pil_images\n",
        "\n",
        "\n",
        "def image_grid(imgs, cols):\n",
        "    grid_w = min([cols, len(imgs)])\n",
        "    grid_h = len(imgs)//cols + 1\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(grid_w*w, grid_h*h))\n",
        "    #grid_w, grid_h = grid.size\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i % cols*w, i//cols*h))\n",
        "\n",
        "    return grid\n",
        "\n",
        "\n",
        "def get_seed_gen(seed):\n",
        "    if seed == 0:\n",
        "      seed = torch.randint(2**32, (1, 1))[0, 0].item()\n",
        "    generator = torch.Generator(device=torch_device)\n",
        "    generator.manual_seed(seed)\n",
        "    return seed, generator\n",
        "\n",
        "\n",
        "def render(steps, scale, generator, embeddings, batch_size):\n",
        "\n",
        "    width = 512\n",
        "    height = 512\n",
        "\n",
        "    scheduler.set_timesteps(steps)\n",
        "\n",
        "\n",
        "    latents = torch.randn(\n",
        "        (batch_size, unet.in_channels, height // 8, width // 8),\n",
        "        generator=generator,\n",
        "        device=torch_device\n",
        "    )\n",
        "    # latents = latents.to(torch_device)  # [batch_size, 4, 64, 64]\n",
        "    latents = latents * scheduler.sigmas[0]\n",
        "\n",
        "    with autocast(\"cuda\"):\n",
        "\n",
        "        for i, t in tqdm(enumerate(scheduler.timesteps), total=steps):\n",
        "\n",
        "            sigma = scheduler.sigmas[i]\n",
        "            latent_model_input = torch.cat(\n",
        "                [latents] * 2)  # [batch_size*2, 4, 64, 64]\n",
        "            latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # [2, 4, 64, 64]\n",
        "                noise_pred = unet(latent_model_input, t,\n",
        "                                  encoder_hidden_states=embeddings).sample\n",
        "\n",
        "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "            noise_pred = noise_pred_uncond + scale * \\\n",
        "                (noise_pred_text - noise_pred_uncond)  # [1, 4, 64, 64]\n",
        "\n",
        "            latents = scheduler.step(\n",
        "                noise_pred, i, latents).prev_sample  # [1, 4, 64, 64]\n",
        "            # print(f\"i={i} t={t}, sigma={sigma}\")\n",
        "\n",
        "    return latents_to_pil(latents)\n",
        "\n",
        "\n",
        "token_EOS_value = 49407\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rI0Kp9QgW6WW"
      },
      "outputs": [],
      "source": [
        "#@markdown #Exploraci√≥n no guiada\n",
        "\n",
        "num_images = 1\n",
        "\n",
        "prompt = \"potruflo, award winning photograph\"  # @param {type:\"string\"}\n",
        "semilla = 498748461  # @param {type:\"number\"}\n",
        "guia = 7.5  # -@param {type:\"slider\", min:4.5, max:21, step:1.5}\n",
        "pasos = 35  # @param {type:\"slider\", min:25, max:100, step:5}\n",
        "\n",
        "\n",
        "prompts = [\"\"] * num_images  # Unconditional\n",
        "prompts.extend([prompt] * num_images)  # Prompts\n",
        "\n",
        "\n",
        "inputs = tokenizer(prompts, padding=\"max_length\", max_length=tokenizer.model_max_length,\n",
        "                   truncation=True, return_tensors=\"pt\")  # [num_images * 2, 77]\n",
        "\n",
        "with torch.no_grad():\n",
        "  text_embeddings = text_encoder(inputs.input_ids.to(torch_device))[\n",
        "      0]  # [num_images * 2, 77, 768]\n",
        "\n",
        "token_list = inputs.input_ids[1].tolist()\n",
        "\n",
        "\n",
        "semilla, generator = get_seed_gen(semilla)\n",
        "images = render(pasos, guia, generator, text_embeddings, num_images)\n",
        "filename = f\"1_{prompt}_{semilla}_{pasos}.jpg\"\n",
        "\n",
        "print(f\"Semilla: {semilla}\")\n",
        "print(f\"Fichero: {filename}\\n\")\n",
        "\n",
        "grid = image_grid(images, cols=3)\n",
        "grid.save(filename, quality=100, subsampling=0)\n",
        "files.download(filename)\n",
        "grid\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuClass": "premium"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3.8.11 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.11"
    },
    "vscode": {
      "interpreter": {
        "hash": "19c52f9406ed8ee88add4472b642b79c35ade567538d579e8a14fbefe9c2ac9c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
